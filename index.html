<!DOCTYPE html>
<html lang="en">

<head>
  <title>Ariel Ephrat</title>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta charset="utf-8">

  <link href='https://fonts.googleapis.com/css?family=Lato:300,400,900' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="mystyle.css" />

 <!-- Google tag (gtag.js) -->
 <script async src="https://www.googletagmanager.com/gtag/js?id=G-C7RBKC34V5"></script>
 <script>
   window.dataLayer = window.dataLayer || [];
   function gtag(){dataLayer.push(arguments);}
   gtag('js', new Date());

   gtag('config', 'G-C7RBKC34V5');
 </script>
</head>

<body>

<!-- Start of StatCounter Code for Default Guide -->
  <script type="text/javascript">
    var sc_project=11225963; 
    var sc_invisible=1; 
    var sc_security="29e62abb"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="real time web
    analytics" href="http://statcounter.com/"
    target="_blank"><img class="statcounter"
    src="//c.statcounter.com/11225963/0/29e62abb/1/" alt="real
    time web analytics"></a></div>
  </noscript>

<div id="html-mobile-background"></div>
<div id="page-wrap">
  <dl>
    <dd>
      <table>
        <tr>
          <td class="profile">
            <img class="profile" src="images/ariel_profile_pic.jpg">
          </td>
          <td class="heading">
            <h1>Ariel Ephrat</h1>
            Research Scientist, Google DeepMind <br />
            Email: arielephrat_at_google.com <br />
			
            <a href = "http://scholar.google.co.il/citations?user=n4dxd1YAAAAJ&hl=en">Google Scholar</a>&nbsp;&nbsp;&nbsp;|
            &nbsp;&nbsp;
            <a href = "https://il.linkedin.com/in/arielephrat">Linkedin</a>&nbsp;&nbsp;&nbsp;|
            &nbsp;&nbsp;
            <a href = "https://github.com/arielephrat">Github</a>
          </td>
        </tr>
      </table>
    </dd>
  </dl>

  <div class="clear"></div>
  <dl>
    <dt>About Me</dt>
    <dd>
		I'm a Research Scientist at <a href = "https://deepmind.google">Google DeepMind</a>, previously on the Perception Team at <a href = "https://ai.google/research/"> Google Research</a>,
	    where I focus on turning generative AI and video understanding into real-world applications. </br>
		</br>
		My work combines academic research with practical impact, contributing to Google's large-scale generative video models: Genie, Veo and Lumiere, as well as product features such as
		Best Take and Speech Mode for Pixel and YouTube Shorts AI effects, alongside notable papers. I completed my PhD in 2018,
		working with <a href="http://www.cs.huji.ac.il/~peleg/">Prof. Shmuel Peleg</a> as part of the <a href = "https://vision.huji.ac.il/">HUJI Machine Perception Group</a>.</br>
		<br />
	    Iâ€™m also drawn to the business side of tech. I've advised <a href = "https://warburgpincus.com">Warburg Pincus</a> on computer vision investments, helping drive two deals,
		and bring perspective from my time as a startup founder. My passion lies in how innovation and value creation come together. </br>
    </dd>
  </dl>

  <div class="clear"></div>
  <dl>
    <dt>Research & Product Highlights</dt>
  
			
			
  		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  			<tr>
  				<td style="padding:2%;width:33%;vertical-align:middle">
  					<a href="https://www.youtube.com/watch?v=PDKhUknuQDg">
  				  <img src='images/genie.png' width="110%" height="auto">
  					</a>
  				</td>
				
  				<td style="padding:2%;width:33%;vertical-align:middle">
  					<a href="https://www.youtube.com/watch?v=dsBzEOli-p0">
  				  <img src='images/best_take.png' width="110%" height="auto">
  					</a>
  				</td>
  				<td style="padding:2%;width:34%;vertical-align:middle">
  				<a href="https://blog.research.google/2018/04/looking-to-listen-audio-visual-speech.html">
  				  <img src='images/l2l_blog.png' width="110%" height="auto">
  					</a>
  				</td>
  			</tr>
  			<tr>
  				<td style="padding:2%;width:33%;vertical-align:middle">
  				  <a href="https://blog.google/products/pixel/snap-faster-hear-better-and-do-more-your-pixel/">
  					<img src='images/ConvoMode_p6_3.gif' width="110%" height="auto"></a>
				  
  				</td>
				
  				<td style="padding:2%;width:33%;vertical-align:middle">
  					<a href="https://blog.google/products/pixel/pixel-6s-camera-combines-hardware-software-and-ml/">
  				  <img src='images/pixel6a.png' width="110%" height="auto">
  					</a>
  				</td>
  				<td style="padding:2%;width:34%;vertical-align:middle">
  				<a href="https://blog.research.google/2018/04/looking-to-listen-audio-visual-speech.html">
  				  <img src='images/l2l_yt_blog.png' width="110%" height="auto">
  					</a>
  				</td>
  		</tbody>
  		</table>
  </dl>
  
  <div class="clear"></div>
  
  <dl>
    <dt>Publications</dt>
    <dd>
      <table>
             <tr>
              <td class="pub_img"><img width=5% src="images/tokenverse.png"></td>
              <td class="pub_desc">
                <span class="title">TokenVerse: Versatile Multi-concept Personalization <br />
 				   in Token Modulation Space</span> <br />
                D. Garibi, S. Yadin, R. Paiss, O. Tov, S. Zada, <b>A. Ephrat</b>, T. Michaeli, I. Mosseri, T. Dekel<br />
                SIGGRAPH (Journal) 2025, <b>Best Paper award</b><br />
                <a href = "https://arxiv.org/abs/2501.12224">arXiv</a>&nbsp;&nbsp;&nbsp;|
                &nbsp;&nbsp;&nbsp;<a href = "https://token-verse.github.io">webpage</a>&nbsp;&nbsp;&nbsp;
              </td>
            </tr>
            <tr class="spacer"><td></td></tr>
			
			<tr>
             <td class="pub_img"><img width=5% src="images/still_moving.jpg"></td>
             <td class="pub_desc">
               <span class="title">Still-Moving: Customized Video Generation <br />
				   without Customized Video Data</span> <br />
               H. Chefer, S. Zada, R. Paiss, <b>A. Ephrat</b>, O. Tov,<br />
 			   M. Rubinstein, L. Wolf, T. Dekel, T. Michaeli, I. Mosseri<br />
               SIGGRAPH Asia (Journal) 2024<br />
               <a href = "https://arxiv.org/abs/2407.08674">arXiv</a>&nbsp;&nbsp;&nbsp;|
               &nbsp;&nbsp;&nbsp;<a href = "https://still-moving.github.io/">webpage</a>&nbsp;&nbsp;&nbsp;|
 	          &nbsp;&nbsp;&nbsp;<a href = "https://www.youtube.com/watch?v=U7UuV_VIjnA">video</a>&nbsp;&nbsp;&nbsp;
			  
             </td>
           </tr>
           <tr class="spacer"><td></td></tr>
		  
           <tr>
            <td class="pub_img"><img width=5% src="images/lumiere.gif"></td>
            <td class="pub_desc">
              <span class="title">Lumiere: A Space-Time Diffusion Model for Video Generation</span> <br />
              O. Bar-Tal, H. Chefer, O. Tov, C. Herrmann, R. Paiss, S. Zada,<br />
			  <b>A. Ephrat</b>, J. Hur, Y. Li, T. Michaeli, O. Wang, D. Sun, T. Dekel, I. Mosseri<br />
              SIGGRAPH Asia, 2024<br />
              <a href = "https://arxiv.org/abs/2401.12945">arXiv</a>&nbsp;&nbsp;&nbsp;|
              &nbsp;&nbsp;&nbsp;<a href = "https://lumiere-video.github.io">webpage</a>&nbsp;&nbsp;&nbsp;|
	          &nbsp;&nbsp;&nbsp;<a href = "https://www.youtube.com/watch?v=wxLr02Dz2Sc">video</a>&nbsp;&nbsp;&nbsp;
			  
            </td>
          </tr>
          <tr class="spacer"><td></td></tr>
		  
		  <tr>
           <td class="pub_img"><img width=5% src="images/clip_count.png"></td>
           <td class="pub_desc">
             <span class="title">Teaching CLIP to Count to Ten</span> <br />
             R. Paiss, <b>A. Ephrat</b>, O. Tov, S. Zada, I. Mosseri, M. Irani, T. Dekel<br />
             ICCV 2023<br />
             <a href = "https://arxiv.org/abs/2302.12066">arXiv</a>&nbsp;&nbsp;&nbsp;|
             &nbsp;&nbsp;&nbsp;<a href = "https://teaching-clip-to-count.github.io">webpage</a>&nbsp;&nbsp;&nbsp;
           </td>
         </tr>
         <tr class="spacer"><td></td></tr>
		 
		 <tr>
          <td class="pub_img"><img width=5% src="images/speednet.png"></td>
          <td class="pub_desc">
            <span class="title">SpeedNet: Learning the Speediness in Videos</span> <br />
            S. Benaim, <b>A. Ephrat</b>, O. Lang, I. Mosseri, W.T. Freeman, M. Rubinstein, M. Irani, T. Dekel<br />
            CVPR 2020, <b>oral</b><br />
            <a href = "https://arxiv.org/abs/2004.06130">arXiv</a>&nbsp;&nbsp;&nbsp;|
            &nbsp;&nbsp;&nbsp;<a href = "http://speednet-cvpr20.github.io/">webpage</a>&nbsp;&nbsp;&nbsp;
          </td>
        </tr>
        <tr class="spacer"><td></td></tr>

         <tr>
          <td class="pub_img"><img width=5% src="images/icml19.png"></td>
          <td class="pub_desc">
            <span class="title">Neural Separation of Observed and Unobserved Distributions</span> <br />
            T. Halperin, <b>A. Ephrat</b>, Y. Hoshen <br />
            ICML 2019<br />
            <a href = "https://arxiv.org/abs/1811.12739">arXiv</a>&nbsp;&nbsp;&nbsp;
          </td>
        </tr>
        <tr class="spacer"><td></td></tr>

        <tr>
          <td class="pub_img"><img width=5% src="images/avsnap.png"></td>
          <td class="pub_desc">
            <span class="title">Dynamic Temporal Alignment of Speech to Lips</span> <br />
            T. Halperin*, <b>A. Ephrat*</b>, S. Peleg <br />
            [* equal contribution] <br />
            ICASSP 2019<br />
            <a href = "http://arxiv.org/abs/1808.06250">arXiv</a>&nbsp;&nbsp;&nbsp;|
	    &nbsp;&nbsp;&nbsp;<a href = "https://www.youtube.com/watch?v=t7m0yEnBG7M">video</a>&nbsp;&nbsp;&nbsp;|
            &nbsp;&nbsp;&nbsp;<a href = "https://github.com/tavihalperin/AV-snap">code</a>&nbsp;&nbsp;
          </td>
        </tr>
        <tr class="spacer"><td></td></tr>

	<tr>
          <td class="pub_img"><img width=5% src="images/l2l_2.png"></td>
          <td class="pub_desc">
            <span class="title">Looking to Listen at the Cocktail Party: <br />
						A Speaker-Independent Audio-Visual Model for Speech Separation</span> <br />
            <b>A. Ephrat</b>, I. Mosseri, O. Lang, T. Dekel, K. Wilson, A. Hassidim, W.T. Freeman, M. Rubinstein <br />
            SIGGRAPH 2018<br />
            <a href = "http://arxiv.org/abs/1804.03619">arXiv</a>&nbsp;&nbsp;&nbsp;|
            &nbsp;&nbsp;&nbsp;<a href = "http://looking-to-listen.github.io/">webpage</a>&nbsp;&nbsp;&nbsp;|
						&nbsp;&nbsp;&nbsp;<a href = "https://www.youtube.com/watch?v=rVQVAPiJWKU">video</a>&nbsp;&nbsp;&nbsp;|
						&nbsp;&nbsp;&nbsp;<a href = "https://research.googleblog.com/2018/04/looking-to-listen-audio-visual-speech.html">Google Research Blog</a>&nbsp;&nbsp;&nbsp;
          </td>
        </tr>
        <tr class="spacer"><td></td></tr>

				<tr>
          <td class="pub_img"><img width=5% src="images/icassp18.png"></td>
          <td class="pub_desc">
            <span class="title">Seeing Through Noise: <br />
						Visually Driven Speaker Separation and Enhancement</span> <br />
            A. Gabbay, <b>A. Ephrat</b>, T. Halperin and S. Peleg <br />
            IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018<br />
            <a href = "https://arxiv.org/abs/1708.06767">arXiv</a>&nbsp;&nbsp;&nbsp;|
            &nbsp;&nbsp;&nbsp;<a href = "http://www.vision.huji.ac.il/speaker-separation/">webpage</a>&nbsp;&nbsp;&nbsp;|
						&nbsp;&nbsp;&nbsp;<a href = "https://www.youtube.com/watch?v=qmsyj7vAzoI">video</a>&nbsp;&nbsp;&nbsp;|
            &nbsp;&nbsp;&nbsp;<a href = "https://github.com/avivga/cocktail-party">code</a>
          </td>
        </tr>
        <tr class="spacer"><td></td></tr>

				<tr>
          <td class="pub_img"><img src="images/livercnn18.png"></td>
          <td class="pub_desc">
            <span class="title">Patient-specific and global convolutional neural networks <br />
						for robust automatic liver tumor delineation in follow-up CT studies</span> <br />
            R. Vivanti, L. Joskowicz, N. Lev Cohain, <b>A. Ephrat</b> and J. Sosna <br />
            Medical & Biological Engineering & Computing, March 2018 <br />
            <a href = "https://link.springer.com/article/10.1007/s11517-018-1803-6">article</a>
          </td>
        </tr>
				<tr class="spacer"><td></td></tr>

        <tr>
          <td class="pub_img"><img src="images/enc_dec_fig_crop.png"></td>
          <td class="pub_desc">
            <span class="title">Improved Speech Reconstruction from Silent Video</span> <br />
            <b>A. Ephrat*</b>, T. Halperin* and S. Peleg <br />
            [* equal contribution] <br />
            ICCV 2017 Workshop on Computer Vision for Audio-Visual Media<br />
            <a href = "https://arxiv.org/abs/1708.01204">arXiv</a>&nbsp;&nbsp;&nbsp;|
            &nbsp;&nbsp;&nbsp;<a href = "http://www.vision.huji.ac.il/vid2speech">webpage</a>&nbsp;&nbsp;&nbsp;|
            &nbsp;&nbsp;&nbsp;<a href = "https://www.youtube.com/watch?v=Xjbn7h7tpg0">video</a>&nbsp;&nbsp;&nbsp;|
            &nbsp;&nbsp;&nbsp;<a href = "https://github.com/arielephrat/vid2speech">code</a>
          </td>
        </tr>
        <tr class="spacer"><td></td></tr>

        <tr>
          <td class="pub_img"><img src="images/vid2speech.png"></td>
          <td class="pub_desc">
            <span class="title">Vid2speech: Speech Reconstruction from Silent Video</span> <br />
            <b>A. Ephrat</b> and S. Peleg <br />
            IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017<br />
            <a href = "./papers/vid2speech_icassp17.pdf">pdf</a>&nbsp;&nbsp;&nbsp;|
            &nbsp;&nbsp;&nbsp;<a href = "http://www.vision.huji.ac.il/vid2speech">webpage</a>&nbsp;&nbsp;&nbsp;|
            &nbsp;&nbsp;&nbsp;<a href = "https://github.com/arielephrat/vid2speech">code</a>
          </td>
        </tr>
        <tr class="spacer"><td></td></tr>

        <tr>
          <td class="pub_img"><img src="images/compactcnn.png"></td>
          <td class="pub_desc">
            <span class="title">Compact CNN for Indexing Egocentric Videos</span> <br />
            Y. Poleg, <b>A. Ephrat</b>, C. Arora and S. Peleg <br />
            IEEE Winter Conference on Applications of Computer Vision (WACV), 2016 <br />
            <a href = "./papers/compactcnn_wacv16.pdf">pdf</a>&nbsp;&nbsp;&nbsp;|
            &nbsp;&nbsp;&nbsp;<a href = "http://www.vision.huji.ac.il/egoseg">webpage</a>
          </td>
        </tr>
        <tr class="spacer"><td></td></tr>

        <tr>
          <td class="pub_img"><img src="images/livercnn15.png"></td>
          <td class="pub_desc">
            <span class="title">Automatic Liver Tumor Segmentation in Follow-Up CT Scans</span> <br />
            R. Vivanti, <b>A. Ephrat</b>, L. Joskowicz, O.A. Karaaslan, N. Lev Cohain and J. Sosna <br />
            Proc. International Workshop on Patch-based Techniques in Medical Imaging, MICCAI, 2015 <br />
            <a href = "./papers/livercnn_miccai15.pdf">pdf</a>
          </td>
        </tr>
      </table>
    </dd>
  </dl>

  <div class="clear"></div>

  <dl>
    <dt>Teaching</dt>
    <dd>
      <table>
        <tr>
          <td><img class="logo" src="images/huji-logo.png"></td>
          <td class="work">
						HUJI &#8212; Teaching Assistant, Digital Image Processing, 2017-18 <br />
            HUJI &#8212; Teaching Assistant, Undergraduate Engineering Projects and Workshop, 2015-18 
          </td>
          <td>
        </tr>
      </table>
    </dd>
  </dl>

  <div class="clear"></div>

  <dl>
    <dt>Work</dt>
    <dd>
      <table>
        <tr>
          <td><img class="logo" src="images/gdm_logo.webp"></td>
          <td class="work">
            Google DeepMind &#8212; Research Scientist, 2024 - Now <br />
          </td>
          <td>
        </tr>
		<tr>
          <td><img class="logo" src="images/google_g_logo.png"></td>
          <td class="work">
            Google Research &#8212; Software Engineer, 2018 - 2024 <br />
			Google Research &#8212; Software Engineering Intern, 2017 <br />
            Google &#8212; Software Engineering Intern, 2016
          </td>
          <td>
        </tr>
        <tr>
          <td><img class="logo" src="images/hereastory_logo.png"></td>
          <td class="work">
            <a href = "http://www.here-a-story.com" target="_blank">Here a Story</a> &#8212; Co-founder and Web Developer, 2013&#8211;2014
          </td>
          <td>
        </tr>
      </table>
    </dd>
     <td>
        <br>
        <!-- <p align="left"> -->
        <font size="1">
   		 Research and product highlights inspired by <a href="https://inbar-mosseri.github.io/">Inbar Mosseri</a>
         </font>
        </p>
    </td>
  </dl>

  <div class="clear"></div>

</div>

</body>
</html>

